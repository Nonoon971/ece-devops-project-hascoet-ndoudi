* 
* ==> Audit <==
* |---------|-----------------|----------|----------------|----------------|---------------------|---------------------|
| Command |      Args       | Profile  |      User      |    Version     |     Start Time      |      End Time       |
|---------|-----------------|----------|----------------|----------------|---------------------|---------------------|
| start   | --driver=docker | minikube | camillehascoet | v1.26.0-beta.1 | 30 Nov 22 10:09 CET | 30 Nov 22 10:11 CET |
| start   | --driver=docker | minikube | camillehascoet | v1.26.0-beta.1 | 26 Dec 22 18:53 CET | 26 Dec 22 18:53 CET |
|---------|-----------------|----------|----------------|----------------|---------------------|---------------------|

* 
* ==> Dernier d√©marrage <==
* Log file created at: 2022/12/27 01:40:11
Running on machine: camillehascoet-ZenBook-UX425EA-UX425EA
Binary: Built with gc go1.18.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1227 01:40:11.211043   23744 out.go:296] Setting OutFile to fd 1 ...
I1227 01:40:11.211096   23744 out.go:348] isatty.IsTerminal(1) = true
I1227 01:40:11.211100   23744 out.go:309] Setting ErrFile to fd 2...
I1227 01:40:11.211103   23744 out.go:348] isatty.IsTerminal(2) = true
I1227 01:40:11.211199   23744 root.go:322] Updating PATH: /home/camillehascoet/.minikube/bin
W1227 01:40:11.211366   23744 root.go:300] Error reading config file at /home/camillehascoet/.minikube/config/config.json: open /home/camillehascoet/.minikube/config/config.json: no such file or directory
I1227 01:40:11.211425   23744 out.go:303] Setting JSON to false
I1227 01:40:11.229469   23744 start.go:115] hostinfo: {"hostname":"camillehascoet-ZenBook-UX425EA-UX425EA","uptime":8675,"bootTime":1672092936,"procs":301,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.0-56-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"bd377773-f75e-43d5-92c4-bd24a960e821"}
I1227 01:40:11.229499   23744 start.go:125] virtualization: kvm host
I1227 01:40:11.231166   23744 out.go:177] üòÑ  minikube v1.26.0-beta.1 sur Ubuntu 22.04
I1227 01:40:11.233460   23744 notify.go:193] Checking for updates...
I1227 01:40:11.233653   23744 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.6
I1227 01:40:11.235297   23744 driver.go:358] Setting default libvirt URI to qemu:///system
I1227 01:40:11.411759   23744 docker.go:137] docker version: linux-20.10.21
I1227 01:40:11.411819   23744 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1227 01:40:11.641981   23744 info.go:265] docker info: {ID:LHOQ:BHUW:G2MB:ED77:UNJR:E55S:T4SG:56K3:DXJI:EVQZ:R54V:QWBP Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:49 OomKillDisable:false NGoroutines:54 SystemTime:2022-12-27 00:40:11.531002695 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:6 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3946115072 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:770bd0108c32f3fb5c73ae1264f7e503fe7b2661 Expected:770bd0108c32f3fb5c73ae1264f7e503fe7b2661} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.13.0] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.16] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-scan] ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.22.0]] Warnings:<nil>}}
I1227 01:40:11.642050   23744 docker.go:254] overlay module found
I1227 01:40:11.643152   23744 out.go:177] ‚ú®  Utilisation du pilote docker bas√© sur le profil existant
I1227 01:40:11.644023   23744 start.go:284] selected driver: docker
I1227 01:40:11.644026   23744 start.go:806] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/camillehascoet:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false}
I1227 01:40:11.644078   23744 start.go:817] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1227 01:40:11.644162   23744 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1227 01:40:11.871178   23744 info.go:265] docker info: {ID:LHOQ:BHUW:G2MB:ED77:UNJR:E55S:T4SG:56K3:DXJI:EVQZ:R54V:QWBP Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:49 OomKillDisable:false NGoroutines:54 SystemTime:2022-12-27 00:40:11.762855608 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:6 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3946115072 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:770bd0108c32f3fb5c73ae1264f7e503fe7b2661 Expected:770bd0108c32f3fb5c73ae1264f7e503fe7b2661} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.13.0] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.16] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-scan] ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.22.0]] Warnings:<nil>}}
I1227 01:40:11.892614   23744 cni.go:95] Creating CNI manager for ""
I1227 01:40:11.892621   23744 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1227 01:40:11.892629   23744 start_flags.go:306] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/camillehascoet:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false}
I1227 01:40:11.893881   23744 out.go:177] üëç  D√©marrage du noeud de plan de contr√¥le minikube dans le cluster minikube
I1227 01:40:11.895517   23744 cache.go:120] Beginning downloading kic base image for docker with docker
I1227 01:40:11.897051   23744 out.go:177] üöú  Extraction de l'image de base...
I1227 01:40:11.899068   23744 preload.go:132] Checking if preload exists for k8s version v1.23.6 and runtime docker
I1227 01:40:11.899087   23744 preload.go:148] Found local preload: /home/camillehascoet/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.6-docker-overlay2-amd64.tar.lz4
I1227 01:40:11.899090   23744 cache.go:57] Caching tarball of preloaded images
I1227 01:40:11.899135   23744 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c in local docker daemon
I1227 01:40:11.899222   23744 preload.go:174] Found /home/camillehascoet/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.6-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1227 01:40:11.899227   23744 cache.go:60] Finished verifying existence of preloaded tar for  v1.23.6 on docker
I1227 01:40:11.899291   23744 profile.go:148] Saving config to /home/camillehascoet/.minikube/profiles/minikube/config.json ...
I1227 01:40:12.072679   23744 cache.go:146] Downloading gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c to local cache
I1227 01:40:12.072808   23744 image.go:59] Checking for gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c in local cache directory
I1227 01:40:12.073189   23744 image.go:62] Found gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c in local cache directory, skipping pull
I1227 01:40:12.073404   23744 image.go:103] gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c exists in cache, skipping pull
I1227 01:40:12.073414   23744 cache.go:149] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c as a tarball
I1227 01:40:12.073416   23744 cache.go:160] Loading gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c from local cache
I1227 01:40:13.393831   23744 cache.go:163] successfully loaded gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c from cached tarball
I1227 01:40:13.393846   23744 cache.go:206] Successfully downloaded all kic artifacts
I1227 01:40:13.393880   23744 start.go:352] acquiring machines lock for minikube: {Name:mk9827aeac64e1eb059ced3c8799cff6de316380 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1227 01:40:13.393988   23744 start.go:356] acquired machines lock for "minikube" in 95.851¬µs
I1227 01:40:13.393997   23744 start.go:94] Skipping create...Using existing machine configuration
I1227 01:40:13.394001   23744 fix.go:55] fixHost starting: 
I1227 01:40:13.394137   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:13.573170   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:13.573204   23744 fix.go:103] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:13.573215   23744 fix.go:108] machineExists: false. err=machine does not exist
I1227 01:40:13.579692   23744 out.go:177] ü§∑  docker "minikube" container est manquant, il va √™tre recr√©√©.
I1227 01:40:13.580692   23744 delete.go:124] DEMOLISHING minikube ...
I1227 01:40:13.580726   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:13.749716   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W1227 01:40:13.749774   23744 stop.go:75] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:13.749797   23744 delete.go:129] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:13.750307   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:13.921408   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:13.921442   23744 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:13.921469   23744 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W1227 01:40:14.089384   23744 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I1227 01:40:14.089435   23744 kic.go:356] could not find the container minikube to remove it. will try anyways
I1227 01:40:14.089519   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:14.261436   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W1227 01:40:14.261488   23744 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:14.261520   23744 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W1227 01:40:14.429209   23744 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I1227 01:40:14.434499   23744 oci.go:625] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error: No such container: minikube
I1227 01:40:15.434804   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:15.614085   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:15.614112   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:15.614116   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:15.614132   23744 retry.go:31] will retry after 552.330144ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:16.166916   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:16.336791   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:16.336857   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:16.336870   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:16.336900   23744 retry.go:31] will retry after 1.080381816s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:17.417693   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:17.589110   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:17.589144   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:17.589149   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:17.589166   23744 retry.go:31] will retry after 1.31013006s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:18.900702   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:19.072861   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:19.072889   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:19.072895   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:19.072908   23744 retry.go:31] will retry after 1.582392691s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:20.656739   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:20.833019   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:20.833149   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:20.833169   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:20.833235   23744 retry.go:31] will retry after 2.340488664s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:23.174602   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:23.349436   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:23.349513   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:23.349535   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:23.349576   23744 retry.go:31] will retry after 4.506218855s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:27.856169   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:28.031005   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:28.031037   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:28.031042   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:28.031060   23744 retry.go:31] will retry after 3.221479586s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:31.252943   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:40:31.428939   23744 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1227 01:40:31.428965   23744 oci.go:637] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
I1227 01:40:31.428969   23744 oci.go:639] temporary error: container minikube status is  but expect it to be exited
I1227 01:40:31.428991   23744 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error: No such container: minikube
 
I1227 01:40:31.429012   23744 cli_runner.go:164] Run: docker rm -f -v minikube
I1227 01:40:31.556874   23744 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W1227 01:40:31.729103   23744 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I1227 01:40:31.729167   23744 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1227 01:40:31.902882   23744 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1227 01:40:31.902918   23744 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I1227 01:40:31.902928   23744 cli_runner.go:164] Run: docker network inspect minikube
W1227 01:40:32.072871   23744 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1227 01:40:32.072885   23744 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1227 01:40:32.072893   23744 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
W1227 01:40:32.073129   23744 delete.go:139] delete failed (probably ok) <nil>
I1227 01:40:32.073133   23744 fix.go:115] Sleeping 1 second for extra luck!
I1227 01:40:33.073366   23744 start.go:131] createHost starting for "" (driver="docker")
I1227 01:40:33.080778   23744 out.go:204] üî•  Cr√©ation de docker container (CPUs=2, Memory=3900Mo) ...
I1227 01:40:33.082006   23744 start.go:165] libmachine.API.Create for "minikube" (driver="docker")
I1227 01:40:33.082037   23744 client.go:168] LocalClient.Create starting
I1227 01:40:33.082642   23744 main.go:134] libmachine: Reading certificate data from /home/camillehascoet/.minikube/certs/ca.pem
I1227 01:40:33.082987   23744 main.go:134] libmachine: Decoding PEM data...
I1227 01:40:33.083019   23744 main.go:134] libmachine: Parsing certificate...
I1227 01:40:33.083148   23744 main.go:134] libmachine: Reading certificate data from /home/camillehascoet/.minikube/certs/cert.pem
I1227 01:40:33.083477   23744 main.go:134] libmachine: Decoding PEM data...
I1227 01:40:33.083495   23744 main.go:134] libmachine: Parsing certificate...
I1227 01:40:33.083812   23744 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1227 01:40:33.257183   23744 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1227 01:40:33.257215   23744 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I1227 01:40:33.257225   23744 cli_runner.go:164] Run: docker network inspect minikube
W1227 01:40:33.431646   23744 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1227 01:40:33.431661   23744 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1227 01:40:33.431668   23744 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I1227 01:40:33.431700   23744 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1227 01:40:33.596706   23744 network.go:240] skipping subnet 192.168.49.0/24 that is taken: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName:br-9e884a8b9ced IfaceIPv4:192.168.49.1 IfaceMTU:1500 IfaceMAC:02:42:4b:82:9a:df}}
I1227 01:40:33.596981   23744 network.go:288] reserving subnet 192.168.58.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.58.0:0xc0000109b0] misses:0}
I1227 01:40:33.596998   23744 network.go:235] using free private subnet 192.168.58.0/24: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I1227 01:40:33.597004   23744 network_create.go:115] attempt to create docker network minikube 192.168.58.0/24 with gateway 192.168.58.1 and MTU of 1500 ...
I1227 01:40:33.597030   23744 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true minikube
I1227 01:40:33.763918   23744 network_create.go:99] docker network minikube 192.168.58.0/24 created
I1227 01:40:33.763935   23744 kic.go:106] calculated static IP "192.168.58.2" for the "minikube" container
I1227 01:40:33.763976   23744 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1227 01:40:33.930367   23744 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1227 01:40:34.052837   23744 oci.go:103] Successfully created a docker volume minikube
I1227 01:40:34.052892   23744 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c -d /var/lib
I1227 01:43:25.797155   23744 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c -d /var/lib: (2m51.744222794s)
I1227 01:43:25.797359   23744 oci.go:107] Successfully prepared a docker volume minikube
I1227 01:43:25.797387   23744 preload.go:132] Checking if preload exists for k8s version v1.23.6 and runtime docker
I1227 01:43:25.797410   23744 kic.go:179] Starting extracting preloaded images to volume ...
I1227 01:43:25.797456   23744 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/camillehascoet/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.6-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c -I lz4 -xf /preloaded.tar -C /extractDir
I1227 01:43:29.478305   23744 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/camillehascoet/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.6-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c -I lz4 -xf /preloaded.tar -C /extractDir: (3.680814852s)
I1227 01:43:29.478322   23744 kic.go:188] duration metric: took 3.680911 seconds to extract preloaded images to volume
W1227 01:43:29.479102   23744 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1227 01:43:29.480481   23744 oci.go:202] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1227 01:43:29.480501   23744 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1227 01:43:29.775029   23744 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.58.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c
I1227 01:43:30.283113   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1227 01:43:30.380429   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 01:43:30.514585   23744 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1227 01:43:30.842736   23744 oci.go:247] the created container "minikube" has a running status.
I1227 01:43:30.846930   23744 kic.go:210] Creating ssh key for kic: /home/camillehascoet/.minikube/machines/minikube/id_rsa...
I1227 01:43:30.888488   23744 kic_runner.go:191] docker (temp): /home/camillehascoet/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1227 01:43:31.140127   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 01:43:31.223308   23744 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1227 01:43:31.226194   23744 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1227 01:43:31.539767   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 01:43:31.711092   23744 machine.go:88] provisioning docker machine ...
I1227 01:43:31.717473   23744 ubuntu.go:169] provisioning hostname "minikube"
I1227 01:43:31.718126   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:31.891070   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:31.903418   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:31.903425   23744 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1227 01:43:32.092911   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1227 01:43:32.093231   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:32.263160   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:32.263259   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:32.263269   23744 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1227 01:43:32.445164   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1227 01:43:32.445185   23744 ubuntu.go:175] set auth options {CertDir:/home/camillehascoet/.minikube CaCertPath:/home/camillehascoet/.minikube/certs/ca.pem CaPrivateKeyPath:/home/camillehascoet/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/camillehascoet/.minikube/machines/server.pem ServerKeyPath:/home/camillehascoet/.minikube/machines/server-key.pem ClientKeyPath:/home/camillehascoet/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/camillehascoet/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/camillehascoet/.minikube}
I1227 01:43:32.445216   23744 ubuntu.go:177] setting up certificates
I1227 01:43:32.445242   23744 provision.go:83] configureAuth start
I1227 01:43:32.445308   23744 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1227 01:43:32.624875   23744 provision.go:138] copyHostCerts
I1227 01:43:32.624905   23744 exec_runner.go:144] found /home/camillehascoet/.minikube/ca.pem, removing ...
I1227 01:43:32.624910   23744 exec_runner.go:207] rm: /home/camillehascoet/.minikube/ca.pem
I1227 01:43:32.624950   23744 exec_runner.go:151] cp: /home/camillehascoet/.minikube/certs/ca.pem --> /home/camillehascoet/.minikube/ca.pem (1099 bytes)
I1227 01:43:32.625001   23744 exec_runner.go:144] found /home/camillehascoet/.minikube/cert.pem, removing ...
I1227 01:43:32.625004   23744 exec_runner.go:207] rm: /home/camillehascoet/.minikube/cert.pem
I1227 01:43:32.625018   23744 exec_runner.go:151] cp: /home/camillehascoet/.minikube/certs/cert.pem --> /home/camillehascoet/.minikube/cert.pem (1143 bytes)
I1227 01:43:32.625051   23744 exec_runner.go:144] found /home/camillehascoet/.minikube/key.pem, removing ...
I1227 01:43:32.625053   23744 exec_runner.go:207] rm: /home/camillehascoet/.minikube/key.pem
I1227 01:43:32.625066   23744 exec_runner.go:151] cp: /home/camillehascoet/.minikube/certs/key.pem --> /home/camillehascoet/.minikube/key.pem (1679 bytes)
I1227 01:43:32.631146   23744 provision.go:112] generating server cert: /home/camillehascoet/.minikube/machines/server.pem ca-key=/home/camillehascoet/.minikube/certs/ca.pem private-key=/home/camillehascoet/.minikube/certs/ca-key.pem org=camillehascoet.minikube san=[192.168.58.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1227 01:43:32.727751   23744 provision.go:172] copyRemoteCerts
I1227 01:43:32.727773   23744 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1227 01:43:32.727791   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:32.897409   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:33.001848   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I1227 01:43:33.032935   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/machines/server.pem --> /etc/docker/server.pem (1224 bytes)
I1227 01:43:33.047805   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1227 01:43:33.063220   23744 provision.go:86] duration metric: configureAuth took 617.963601ms
I1227 01:43:33.063232   23744 ubuntu.go:193] setting minikube options for container-runtime
I1227 01:43:33.066991   23744 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.6
I1227 01:43:33.067016   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:33.237892   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:33.238001   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:33.238007   23744 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1227 01:43:33.422346   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1227 01:43:33.422365   23744 ubuntu.go:71] root file system type: overlay
I1227 01:43:33.425278   23744 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1227 01:43:33.425341   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:33.596827   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:33.608960   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:33.609000   23744 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1227 01:43:33.844992   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1227 01:43:33.856954   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:34.033760   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:34.033855   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:34.033865   23744 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1227 01:43:34.690920   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-05-05 13:17:28.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-12-27 00:43:33.799300000 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1227 01:43:34.690939   23744 machine.go:91] provisioned docker machine in 2.979838167s
I1227 01:43:34.690945   23744 client.go:171] LocalClient.Create took 3m1.608904164s
I1227 01:43:34.690956   23744 start.go:173] duration metric: libmachine.API.Create for "minikube" took 3m1.608955876s
I1227 01:43:34.690967   23744 start.go:306] post-start starting for "minikube" (driver="docker")
I1227 01:43:34.690970   23744 start.go:316] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1227 01:43:34.691010   23744 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1227 01:43:34.691036   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:34.873680   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:34.976881   23744 ssh_runner.go:195] Run: cat /etc/os-release
I1227 01:43:34.985023   23744 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1227 01:43:34.985045   23744 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1227 01:43:34.985058   23744 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1227 01:43:34.985065   23744 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I1227 01:43:34.985076   23744 filesync.go:126] Scanning /home/camillehascoet/.minikube/addons for local assets ...
I1227 01:43:34.990757   23744 filesync.go:126] Scanning /home/camillehascoet/.minikube/files for local assets ...
I1227 01:43:34.991004   23744 start.go:309] post-start completed in 300.029205ms
I1227 01:43:34.991330   23744 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1227 01:43:35.119294   23744 profile.go:148] Saving config to /home/camillehascoet/.minikube/profiles/minikube/config.json ...
I1227 01:43:35.119447   23744 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1227 01:43:35.119468   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:35.290101   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:35.428795   23744 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1227 01:43:35.437046   23744 start.go:134] duration metric: createHost completed in 3m2.36366153s
I1227 01:43:35.437084   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1227 01:43:35.565197   23744 fix.go:129] unexpected machine state, will restart: <nil>
I1227 01:43:35.565236   23744 machine.go:88] provisioning docker machine ...
I1227 01:43:35.565276   23744 ubuntu.go:169] provisioning hostname "minikube"
I1227 01:43:35.565335   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:35.738457   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:35.738538   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:35.738543   23744 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1227 01:43:35.993763   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1227 01:43:35.993827   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:36.168342   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:36.168449   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:36.168459   23744 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1227 01:43:36.353811   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1227 01:43:36.353837   23744 ubuntu.go:175] set auth options {CertDir:/home/camillehascoet/.minikube CaCertPath:/home/camillehascoet/.minikube/certs/ca.pem CaPrivateKeyPath:/home/camillehascoet/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/camillehascoet/.minikube/machines/server.pem ServerKeyPath:/home/camillehascoet/.minikube/machines/server-key.pem ClientKeyPath:/home/camillehascoet/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/camillehascoet/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/camillehascoet/.minikube}
I1227 01:43:36.353858   23744 ubuntu.go:177] setting up certificates
I1227 01:43:36.353867   23744 provision.go:83] configureAuth start
I1227 01:43:36.353922   23744 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1227 01:43:36.530275   23744 provision.go:138] copyHostCerts
I1227 01:43:36.530354   23744 exec_runner.go:144] found /home/camillehascoet/.minikube/key.pem, removing ...
I1227 01:43:36.530357   23744 exec_runner.go:207] rm: /home/camillehascoet/.minikube/key.pem
I1227 01:43:36.530395   23744 exec_runner.go:151] cp: /home/camillehascoet/.minikube/certs/key.pem --> /home/camillehascoet/.minikube/key.pem (1679 bytes)
I1227 01:43:36.530433   23744 exec_runner.go:144] found /home/camillehascoet/.minikube/ca.pem, removing ...
I1227 01:43:36.530435   23744 exec_runner.go:207] rm: /home/camillehascoet/.minikube/ca.pem
I1227 01:43:36.530450   23744 exec_runner.go:151] cp: /home/camillehascoet/.minikube/certs/ca.pem --> /home/camillehascoet/.minikube/ca.pem (1099 bytes)
I1227 01:43:36.530478   23744 exec_runner.go:144] found /home/camillehascoet/.minikube/cert.pem, removing ...
I1227 01:43:36.530480   23744 exec_runner.go:207] rm: /home/camillehascoet/.minikube/cert.pem
I1227 01:43:36.530493   23744 exec_runner.go:151] cp: /home/camillehascoet/.minikube/certs/cert.pem --> /home/camillehascoet/.minikube/cert.pem (1143 bytes)
I1227 01:43:36.530513   23744 provision.go:112] generating server cert: /home/camillehascoet/.minikube/machines/server.pem ca-key=/home/camillehascoet/.minikube/certs/ca.pem private-key=/home/camillehascoet/.minikube/certs/ca-key.pem org=camillehascoet.minikube san=[192.168.58.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1227 01:43:36.592479   23744 provision.go:172] copyRemoteCerts
I1227 01:43:36.592514   23744 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1227 01:43:36.592530   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:36.723386   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:36.825167   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/machines/server.pem --> /etc/docker/server.pem (1220 bytes)
I1227 01:43:36.840150   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1227 01:43:36.853707   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I1227 01:43:36.867594   23744 provision.go:86] duration metric: configureAuth took 513.720405ms
I1227 01:43:36.867604   23744 ubuntu.go:193] setting minikube options for container-runtime
I1227 01:43:36.867764   23744 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.6
I1227 01:43:36.867795   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:37.039091   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:37.039195   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:37.039200   23744 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1227 01:43:37.221929   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1227 01:43:37.221946   23744 ubuntu.go:71] root file system type: overlay
I1227 01:43:37.222205   23744 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1227 01:43:37.222278   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:37.404233   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:37.404320   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:37.404362   23744 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1227 01:43:37.653050   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1227 01:43:37.653125   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:37.831435   23744 main.go:134] libmachine: Using SSH client type: native
I1227 01:43:37.831539   23744 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7da240] 0x7dd2a0 <nil>  [] 0s} 127.0.0.1 42843 <nil> <nil>}
I1227 01:43:37.831551   23744 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1227 01:43:38.017942   23744 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1227 01:43:38.017961   23744 machine.go:91] provisioned docker machine in 2.452716051s
I1227 01:43:38.017970   23744 start.go:306] post-start starting for "minikube" (driver="docker")
I1227 01:43:38.017978   23744 start.go:316] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1227 01:43:38.018043   23744 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1227 01:43:38.018088   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:38.153356   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:38.254415   23744 ssh_runner.go:195] Run: cat /etc/os-release
I1227 01:43:38.259005   23744 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1227 01:43:38.259017   23744 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1227 01:43:38.259025   23744 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1227 01:43:38.259030   23744 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I1227 01:43:38.259038   23744 filesync.go:126] Scanning /home/camillehascoet/.minikube/addons for local assets ...
I1227 01:43:38.259077   23744 filesync.go:126] Scanning /home/camillehascoet/.minikube/files for local assets ...
I1227 01:43:38.259088   23744 start.go:309] post-start completed in 241.111925ms
I1227 01:43:38.259112   23744 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1227 01:43:38.259129   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:38.434977   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:38.564542   23744 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1227 01:43:38.571541   23744 fix.go:57] fixHost completed within 3m25.177536735s
I1227 01:43:38.571549   23744 start.go:81] releasing machines lock for "minikube", held for 3m25.177556854s
I1227 01:43:38.571603   23744 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1227 01:43:38.740534   23744 ssh_runner.go:195] Run: systemctl --version
I1227 01:43:38.740556   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:38.740575   23744 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1227 01:43:38.740599   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:43:38.880585   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:38.885199   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:43:39.250886   23744 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1227 01:43:39.261422   23744 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1227 01:43:39.312563   23744 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1227 01:43:39.312612   23744 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1227 01:43:39.323664   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I1227 01:43:39.334977   23744 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1227 01:43:39.393533   23744 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1227 01:43:39.451543   23744 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1227 01:43:39.500948   23744 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1227 01:43:39.570763   23744 ssh_runner.go:195] Run: sudo systemctl start docker
I1227 01:43:39.579488   23744 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1227 01:43:39.610824   23744 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1227 01:43:39.644208   23744 out.go:204] üê≥  Pr√©paration de Kubernetes v1.23.6 sur Docker 20.10.15...
I1227 01:43:39.644261   23744 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1227 01:43:39.766679   23744 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I1227 01:43:39.816561   23744 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1227 01:43:39.826138   23744 preload.go:132] Checking if preload exists for k8s version v1.23.6 and runtime docker
I1227 01:43:39.826171   23744 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1227 01:43:39.853933   23744 docker.go:610] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.23.6
k8s.gcr.io/kube-proxy:v1.23.6
k8s.gcr.io/kube-scheduler:v1.23.6
k8s.gcr.io/kube-controller-manager:v1.23.6
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1227 01:43:39.853944   23744 docker.go:541] Images already preloaded, skipping extraction
I1227 01:43:39.853987   23744 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1227 01:43:39.882867   23744 docker.go:610] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.23.6
k8s.gcr.io/kube-proxy:v1.23.6
k8s.gcr.io/kube-scheduler:v1.23.6
k8s.gcr.io/kube-controller-manager:v1.23.6
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1227 01:43:39.882879   23744 cache_images.go:84] Images are preloaded, skipping loading
I1227 01:43:39.882909   23744 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1227 01:43:39.945730   23744 cni.go:95] Creating CNI manager for ""
I1227 01:43:39.945740   23744 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1227 01:43:39.946166   23744 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1227 01:43:39.946608   23744 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.23.6 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1227 01:43:39.947150   23744 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.23.6
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1227 01:43:39.947196   23744 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.23.6/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1227 01:43:39.947223   23744 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.23.6
I1227 01:43:39.955474   23744 binaries.go:44] Found k8s binaries, skipping transfer
I1227 01:43:39.955516   23744 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1227 01:43:39.961629   23744 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (334 bytes)
I1227 01:43:39.971835   23744 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1227 01:43:39.982392   23744 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2029 bytes)
I1227 01:43:39.992527   23744 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I1227 01:43:40.040562   23744 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1227 01:43:40.052101   23744 certs.go:54] Setting up /home/camillehascoet/.minikube/profiles/minikube for IP: 192.168.58.2
I1227 01:43:40.052250   23744 certs.go:182] skipping minikubeCA CA generation: /home/camillehascoet/.minikube/ca.key
I1227 01:43:40.060160   23744 certs.go:182] skipping proxyClientCA CA generation: /home/camillehascoet/.minikube/proxy-client-ca.key
I1227 01:43:40.060214   23744 certs.go:298] skipping minikube-user signed cert generation: /home/camillehascoet/.minikube/profiles/minikube/client.key
I1227 01:43:40.060225   23744 certs.go:302] generating minikube signed cert: /home/camillehascoet/.minikube/profiles/minikube/apiserver.key.cee25041
I1227 01:43:40.060772   23744 crypto.go:68] Generating cert /home/camillehascoet/.minikube/profiles/minikube/apiserver.crt.cee25041 with IP's: [192.168.58.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1227 01:43:40.230540   23744 crypto.go:156] Writing cert to /home/camillehascoet/.minikube/profiles/minikube/apiserver.crt.cee25041 ...
I1227 01:43:40.230550   23744 lock.go:35] WriteFile acquiring /home/camillehascoet/.minikube/profiles/minikube/apiserver.crt.cee25041: {Name:mkc55a6084f58a77e67b57a23697167eb25526f3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1227 01:43:40.230680   23744 crypto.go:164] Writing key to /home/camillehascoet/.minikube/profiles/minikube/apiserver.key.cee25041 ...
I1227 01:43:40.230683   23744 lock.go:35] WriteFile acquiring /home/camillehascoet/.minikube/profiles/minikube/apiserver.key.cee25041: {Name:mk429489aec08ed32a64d9844ceb61812ac41746 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1227 01:43:40.230721   23744 certs.go:320] copying /home/camillehascoet/.minikube/profiles/minikube/apiserver.crt.cee25041 -> /home/camillehascoet/.minikube/profiles/minikube/apiserver.crt
I1227 01:43:40.230783   23744 certs.go:324] copying /home/camillehascoet/.minikube/profiles/minikube/apiserver.key.cee25041 -> /home/camillehascoet/.minikube/profiles/minikube/apiserver.key
I1227 01:43:40.240061   23744 certs.go:298] skipping aggregator signed cert generation: /home/camillehascoet/.minikube/profiles/minikube/proxy-client.key
I1227 01:43:40.240119   23744 certs.go:388] found cert: /home/camillehascoet/.minikube/certs/home/camillehascoet/.minikube/certs/ca-key.pem (1675 bytes)
I1227 01:43:40.240133   23744 certs.go:388] found cert: /home/camillehascoet/.minikube/certs/home/camillehascoet/.minikube/certs/ca.pem (1099 bytes)
I1227 01:43:40.240144   23744 certs.go:388] found cert: /home/camillehascoet/.minikube/certs/home/camillehascoet/.minikube/certs/cert.pem (1143 bytes)
I1227 01:43:40.240156   23744 certs.go:388] found cert: /home/camillehascoet/.minikube/certs/home/camillehascoet/.minikube/certs/key.pem (1679 bytes)
I1227 01:43:40.242164   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1227 01:43:40.260686   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1227 01:43:40.275552   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1227 01:43:40.289863   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1227 01:43:40.303893   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1227 01:43:40.317895   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1227 01:43:40.332489   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1227 01:43:40.347104   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1227 01:43:40.366674   23744 ssh_runner.go:362] scp /home/camillehascoet/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1227 01:43:40.381249   23744 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (752 bytes)
I1227 01:43:40.392320   23744 ssh_runner.go:195] Run: openssl version
I1227 01:43:40.397766   23744 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1227 01:43:40.405146   23744 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1227 01:43:40.408972   23744 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Nov 30 09:11 /usr/share/ca-certificates/minikubeCA.pem
I1227 01:43:40.409003   23744 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1227 01:43:40.413321   23744 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1227 01:43:40.421494   23744 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/camillehascoet:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false}
I1227 01:43:40.421596   23744 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1227 01:43:40.446243   23744 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1227 01:43:40.496700   23744 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1227 01:43:40.506165   23744 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I1227 01:43:40.506195   23744 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1227 01:43:40.556529   23744 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1227 01:43:40.556565   23744 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.6:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1227 01:43:48.903756   23744 out.go:204]     ‚ñ™ G√©n√©ration des certificats et des cl√©s
I1227 01:43:48.905673   23744 out.go:204]     ‚ñ™ D√©marrage du plan de contr√¥le ...
I1227 01:43:48.908271   23744 out.go:204]     ‚ñ™ Configuration des r√®gles RBAC ...
I1227 01:43:48.911301   23744 cni.go:95] Creating CNI manager for ""
I1227 01:43:48.911313   23744 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1227 01:43:48.911349   23744 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1227 01:43:48.911408   23744 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.23.6/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1227 01:43:48.911406   23744 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.23.6/kubectl label nodes minikube.k8s.io/version=v1.26.0-beta.1 minikube.k8s.io/commit=f0a6d95bdb2f2b83c4f952383fe29de03c269eab minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2022_12_27T01_43_48_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1227 01:43:48.923874   23744 ops.go:34] apiserver oom_adj: -16
I1227 01:43:48.981228   23744 kubeadm.go:1045] duration metric: took 69.872675ms to wait for elevateKubeSystemPrivileges.
I1227 01:43:49.180342   23744 kubeadm.go:397] StartCluster complete in 8.758851143s
I1227 01:43:49.180359   23744 settings.go:142] acquiring lock: {Name:mkf67f8f6eac07c396a6204dcf0e65fd695a4f6c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1227 01:43:49.180466   23744 settings.go:150] Updating kubeconfig:  /home/camillehascoet/.kube/config
I1227 01:43:49.181208   23744 lock.go:35] WriteFile acquiring /home/camillehascoet/.kube/config: {Name:mk7dbaa62c032aae39246ad550fe29a883c6b10f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
W1227 01:44:19.189180   23744 kapi.go:226] failed getting deployment scale, will retry: Get "https://192.168.58.2:8443/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp 192.168.58.2:8443: i/o timeout
W1227 01:44:49.691444   23744 kapi.go:226] failed getting deployment scale, will retry: Get "https://192.168.58.2:8443/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp 192.168.58.2:8443: i/o timeout
W1227 01:45:20.190622   23744 kapi.go:226] failed getting deployment scale, will retry: Get "https://192.168.58.2:8443/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp 192.168.58.2:8443: i/o timeout
W1227 01:45:50.691038   23744 kapi.go:226] failed getting deployment scale, will retry: Get "https://192.168.58.2:8443/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp 192.168.58.2:8443: i/o timeout
W1227 01:46:21.191101   23744 kapi.go:226] failed getting deployment scale, will retry: Get "https://192.168.58.2:8443/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp 192.168.58.2:8443: i/o timeout
W1227 01:46:51.192237   23744 kapi.go:226] failed getting deployment scale, will retry: Get "https://192.168.58.2:8443/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp 192.168.58.2:8443: i/o timeout
I1227 01:46:51.192265   23744 kapi.go:241] timed out trying to rescale deployment "coredns" in namespace "kube-system" and context "minikube" to 1: timed out waiting for the condition
E1227 01:46:51.192276   23744 start.go:264] Unable to scale down deployment "coredns" in namespace "kube-system" to 1 replica: timed out waiting for the condition
I1227 01:46:51.192356   23744 start.go:208] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}
I1227 01:46:51.198399   23744 out.go:177] üîé  V√©rification des composants Kubernetes...
I1227 01:46:51.192483   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1227 01:46:51.192492   23744 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1227 01:46:51.192821   23744 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.6
I1227 01:46:51.199770   23744 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1227 01:46:51.199782   23744 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1227 01:46:51.199787   23744 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1227 01:46:51.199807   23744 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I1227 01:46:51.199812   23744 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W1227 01:46:51.199820   23744 addons.go:165] addon storage-provisioner should already be in state true
I1227 01:46:51.200310   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 01:46:51.200359   23744 host.go:66] Checking if "minikube" exists ...
I1227 01:46:51.200980   23744 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 01:46:51.293080   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.58.1 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.23.6/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1227 01:46:51.294452   23744 api_server.go:51] waiting for apiserver process to appear ...
I1227 01:46:51.294509   23744 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1227 01:46:51.346211   23744 out.go:177]     ‚ñ™ Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I1227 01:46:51.347172   23744 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1227 01:46:51.347180   23744 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1227 01:46:51.347217   23744 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 01:46:51.518457   23744 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:42843 SSHKeyPath:/home/camillehascoet/.minikube/machines/minikube/id_rsa Username:docker}
I1227 01:46:51.645546   23744 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.6/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1227 01:46:51.765486   23744 start.go:806] {"host.minikube.internal": 192.168.58.1} host record injected into CoreDNS
I1227 01:46:51.765508   23744 api_server.go:71] duration metric: took 573.126857ms to wait for apiserver process to appear ...
I1227 01:46:51.765519   23744 api_server.go:87] waiting for apiserver healthz status ...
I1227 01:46:51.765526   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:46:56.766196   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:46:57.267158   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:02.268171   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:02.766902   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:07.767143   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:08.266630   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:13.266876   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:13.266930   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:18.267159   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:18.267254   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
W1227 01:47:21.344566   23744 out.go:239] ‚ùó  L'activation de 'default-storageclass' a renvoy√© une erreur : running callbacks: [Error making standard the default storage class: Error listing StorageClasses: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses": dial tcp 192.168.58.2:8443: i/o timeout]
I1227 01:47:21.350833   23744 out.go:177] üåü  Modules activ√©s: storage-provisioner
I1227 01:47:21.351916   23744 addons.go:417] enableAddons completed in 30.159439754s
I1227 01:47:23.267729   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:23.766318   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:28.767251   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:29.267331   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:34.268559   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:34.767320   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:39.768311   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:40.266976   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:45.267821   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:45.766545   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:50.766986   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:47:51.267173   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:47:51.344505   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:47:51.344567   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:47:51.369870   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:47:51.369910   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:47:51.395390   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:47:51.395473   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:47:51.423723   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:47:51.423762   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:47:51.447258   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:47:51.447306   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:47:51.473039   23744 logs.go:274] 0 containers: []
W1227 01:47:51.473051   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:47:51.473083   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:47:51.496958   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:47:51.497002   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:47:51.521774   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:47:51.521792   23744 logs.go:123] Gathering logs for container status ...
I1227 01:47:51.521799   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:47:51.544406   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:47:51.544417   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:47:51.643042   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:47:51.643068   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:47:51.671718   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:47:51.671730   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:47:51.738411   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:47:51.738431   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:47:51.817496   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:47:51.817507   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:47:51.883544   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:47:51.883563   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:47:51.960079   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:47:51.960087   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:47:51.993058   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:47:51.993070   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:47:52.122696   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:47:52.122704   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:47:52.132492   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:47:52.132504   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:47:52.211051   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:47:52.211066   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:47:52.292461   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:47:52.292480   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:47:54.822367   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:47:59.822833   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:48:00.266505   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:48:00.299379   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:48:00.299416   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:48:00.324659   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:48:00.324695   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:48:00.349604   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:48:00.349638   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:48:00.412587   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:48:00.412646   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:48:00.480461   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:48:00.480527   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:48:00.505922   23744 logs.go:274] 0 containers: []
W1227 01:48:00.505930   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:48:00.505954   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:48:00.531201   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:48:00.531238   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:48:00.556275   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:48:00.556288   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:48:00.556294   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:48:00.630279   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:48:00.630289   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:48:00.702965   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:48:00.702974   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:48:00.729772   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:48:00.729784   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:48:00.798263   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:48:00.798272   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:48:00.826928   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:48:00.826940   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:48:00.952432   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:48:00.952444   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:48:01.050744   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:48:01.050765   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:48:01.084089   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:48:01.084099   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:48:01.164722   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:48:01.164733   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:48:01.223802   23744 logs.go:123] Gathering logs for container status ...
I1227 01:48:01.223820   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:48:01.293996   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:48:01.294014   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:48:01.304517   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:48:01.304526   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:48:03.831074   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:48:08.831374   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:48:09.267202   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:48:09.294710   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:48:09.294743   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:48:09.319191   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:48:09.319227   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:48:09.344600   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:48:09.344650   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:48:09.369195   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:48:09.369233   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:48:09.393356   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:48:09.393408   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:48:09.418319   23744 logs.go:274] 0 containers: []
W1227 01:48:09.418328   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:48:09.418354   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:48:09.443717   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:48:09.443751   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:48:09.468425   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:48:09.468444   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:48:09.468453   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:48:09.543021   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:48:09.543030   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:48:09.615570   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:48:09.615579   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:48:09.642843   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:48:09.642854   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:48:09.710343   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:48:09.710361   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:48:09.778014   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:48:09.778032   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:48:09.854248   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:48:09.854258   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:48:09.986866   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:48:09.986874   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:48:09.996708   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:48:09.996717   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:48:10.098710   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:48:10.098718   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:48:10.130551   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:48:10.130562   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:48:10.204177   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:48:10.204186   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:48:10.260700   23744 logs.go:123] Gathering logs for container status ...
I1227 01:48:10.260710   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:48:12.783102   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:48:17.783702   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:48:18.267362   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:48:18.304298   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:48:18.304337   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:48:18.332557   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:48:18.332593   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:48:18.363620   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:48:18.363668   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:48:18.399197   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:48:18.399230   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:48:18.429354   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:48:18.429562   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:48:18.459703   23744 logs.go:274] 0 containers: []
W1227 01:48:18.459713   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:48:18.459760   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:48:18.490611   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:48:18.490654   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:48:18.519340   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:48:18.519355   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:48:18.519361   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:48:18.549225   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:48:18.549241   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:48:18.617053   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:48:18.617079   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:48:18.684862   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:48:18.684874   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:48:18.796010   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:48:18.796026   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:48:18.880537   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:48:18.880548   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:48:18.954269   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:48:18.954279   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:48:19.033544   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:48:19.033555   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:48:19.108358   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:48:19.108368   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:48:19.169302   23744 logs.go:123] Gathering logs for container status ...
I1227 01:48:19.169312   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:48:19.237510   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:48:19.237525   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:48:19.322668   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:48:19.322679   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:48:19.331600   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:48:19.331610   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:48:21.863700   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:48:26.864765   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:48:27.267344   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:48:27.336664   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:48:27.336729   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:48:27.373357   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:48:27.373410   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:48:27.405558   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:48:27.405594   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:48:27.472515   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:48:27.472590   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:48:27.540593   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:48:27.540677   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:48:27.573774   23744 logs.go:274] 0 containers: []
W1227 01:48:27.573786   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:48:27.573829   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:48:27.600838   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:48:27.600880   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:48:27.627925   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:48:27.627942   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:48:27.627948   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:48:27.700468   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:48:27.700478   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:48:27.730727   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:48:27.730739   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:48:27.766580   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:48:27.766598   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:48:27.838357   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:48:27.838379   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:48:27.922330   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:48:27.922340   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:48:27.977502   23744 logs.go:123] Gathering logs for container status ...
I1227 01:48:27.977511   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:48:28.002409   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:48:28.002420   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:48:28.132814   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:48:28.132826   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:48:28.144507   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:48:28.144517   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:48:28.253970   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:48:28.253983   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:48:28.327981   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:48:28.327996   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:48:28.359312   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:48:28.359324   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:48:30.929637   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:48:35.929884   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:48:36.266603   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:48:36.344572   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:48:36.344638   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:48:36.412617   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:48:36.412701   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:48:36.446289   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:48:36.446321   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:48:36.472962   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:48:36.473012   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:48:36.502382   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:48:36.502423   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:48:36.534389   23744 logs.go:274] 0 containers: []
W1227 01:48:36.534401   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:48:36.534435   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:48:36.576316   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:48:36.576351   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:48:36.605750   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:48:36.605765   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:48:36.605776   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:48:36.672541   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:48:36.672552   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:48:36.703501   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:48:36.703511   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:48:36.778916   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:48:36.778933   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:48:36.837644   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:48:36.837656   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:48:36.920888   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:48:36.920899   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:48:37.022973   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:48:37.022988   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:48:37.102898   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:48:37.102910   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:48:37.181245   23744 logs.go:123] Gathering logs for container status ...
I1227 01:48:37.181259   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:48:37.208698   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:48:37.208708   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:48:37.219956   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:48:37.219971   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:48:37.292990   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:48:37.293000   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:48:37.322363   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:48:37.322378   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:48:39.902113   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:48:44.902733   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:48:45.267343   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:48:45.302979   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:48:45.303022   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:48:45.328522   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:48:45.328558   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:48:45.357292   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:48:45.357331   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:48:45.424360   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:48:45.424398   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:48:45.452357   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:48:45.452395   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:48:45.477451   23744 logs.go:274] 0 containers: []
W1227 01:48:45.477462   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:48:45.477491   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:48:45.503338   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:48:45.503372   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:48:45.531146   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:48:45.531160   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:48:45.531166   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:48:45.590266   23744 logs.go:123] Gathering logs for container status ...
I1227 01:48:45.590276   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:48:45.658027   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:48:45.658046   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:48:45.784345   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:48:45.784355   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:48:45.856895   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:48:45.856905   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:48:45.885514   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:48:45.885526   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:48:45.912326   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:48:45.912336   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:48:45.985742   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:48:45.985759   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:48:46.062832   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:48:46.062842   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:48:46.072145   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:48:46.072156   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:48:46.178309   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:48:46.178318   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:48:46.251677   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:48:46.251687   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:48:46.326330   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:48:46.326340   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:48:48.893815   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:48:53.894202   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:48:54.266721   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:48:54.295612   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:48:54.295654   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:48:54.322326   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:48:54.322364   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:48:54.353070   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:48:54.353115   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:48:54.380284   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:48:54.380322   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:48:54.407231   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:48:54.407273   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:48:54.433547   23744 logs.go:274] 0 containers: []
W1227 01:48:54.433555   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:48:54.433581   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:48:54.458974   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:48:54.459015   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:48:54.484280   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:48:54.484293   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:48:54.484299   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:48:54.537190   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:48:54.537216   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:48:54.565914   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:48:54.565930   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:48:54.639160   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:48:54.639170   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:48:54.706162   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:48:54.706178   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:48:54.776887   23744 logs.go:123] Gathering logs for container status ...
I1227 01:48:54.776897   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:48:54.801108   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:48:54.801118   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:48:54.883874   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:48:54.883883   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:48:54.981898   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:48:54.981908   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:48:55.054968   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:48:55.054977   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:48:55.132569   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:48:55.132580   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:48:55.201501   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:48:55.201524   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:48:55.296725   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:48:55.296738   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:48:57.852376   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:49:02.853142   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:49:03.266849   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:49:03.344570   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:49:03.344657   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:49:03.373952   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:49:03.373987   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:49:03.436251   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:49:03.436287   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:49:03.465551   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:49:03.465589   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:49:03.490171   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:49:03.490247   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:49:03.514874   23744 logs.go:274] 0 containers: []
W1227 01:49:03.514883   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:49:03.514912   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:49:03.540190   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:49:03.540261   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:49:03.565348   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:49:03.565365   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:49:03.565371   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:49:03.638700   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:49:03.638720   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:49:03.721539   23744 logs.go:123] Gathering logs for container status ...
I1227 01:49:03.721548   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:49:03.744877   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:49:03.744889   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:49:03.824476   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:49:03.824486   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:49:03.834354   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:49:03.834364   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:49:03.934489   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:49:03.934509   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:49:04.001263   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:49:04.001280   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:49:04.086570   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:49:04.086587   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:49:04.162185   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:49:04.162195   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:49:04.239065   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:49:04.239075   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:49:04.268259   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:49:04.268273   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:49:04.338169   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:49:04.338188   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:49:06.898303   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:49:11.899530   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:49:12.267084   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:49:12.294158   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:49:12.294199   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:49:12.319142   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:49:12.319188   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:49:12.384414   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:49:12.384562   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:49:12.411732   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:49:12.411766   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:49:12.439296   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:49:12.439335   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:49:12.464346   23744 logs.go:274] 0 containers: []
W1227 01:49:12.464356   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:49:12.464381   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:49:12.490759   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:49:12.490805   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:49:12.518324   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:49:12.518341   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:49:12.518348   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:49:12.593133   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:49:12.593142   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:49:12.664900   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:49:12.664911   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:49:12.694101   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:49:12.694114   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:49:12.765944   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:49:12.765957   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:49:12.841757   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:49:12.841768   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:49:12.895799   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:49:12.895808   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:49:12.994517   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:49:12.994527   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:49:13.004825   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:49:13.004837   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:49:13.031742   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:49:13.031753   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:49:13.105393   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:49:13.105405   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:49:13.173859   23744 logs.go:123] Gathering logs for container status ...
I1227 01:49:13.173877   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:49:13.198484   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:49:13.198494   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:49:15.782790   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:49:20.783016   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:49:21.267318   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:49:21.336459   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:49:21.336522   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:49:21.365000   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:49:21.365046   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:49:21.391103   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:49:21.391139   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:49:21.418475   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:49:21.418508   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:49:21.445341   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:49:21.445386   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:49:21.469907   23744 logs.go:274] 0 containers: []
W1227 01:49:21.469920   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:49:21.469961   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:49:21.495241   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:49:21.495318   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:49:21.520419   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:49:21.520441   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:49:21.520450   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:49:21.604665   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:49:21.604674   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:49:21.615168   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:49:21.615181   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:49:21.682219   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:49:21.682238   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:49:21.741427   23744 logs.go:123] Gathering logs for container status ...
I1227 01:49:21.741436   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:49:21.765126   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:49:21.765136   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:49:21.850611   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:49:21.850622   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:49:21.949399   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:49:21.949409   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:49:22.021320   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:49:22.021329   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:49:22.100134   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:49:22.100148   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:49:22.132284   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:49:22.132298   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:49:22.158629   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:49:22.158641   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:49:22.234746   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:49:22.234756   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:49:24.803226   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:49:29.803737   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:49:30.266552   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:49:30.340510   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:49:30.340577   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:49:30.369956   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:49:30.369990   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:49:30.394500   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:49:30.394549   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:49:30.419804   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:49:30.419845   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:49:30.445321   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:49:30.445361   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:49:30.469923   23744 logs.go:274] 0 containers: []
W1227 01:49:30.469932   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:49:30.469959   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:49:30.544165   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:49:30.544200   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:49:30.568954   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:49:30.568976   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:49:30.568996   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:49:30.577855   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:49:30.577865   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:49:30.652562   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:49:30.652570   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:49:30.679024   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:49:30.679038   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:49:30.753404   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:49:30.753415   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:49:30.821874   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:49:30.821893   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:49:30.887533   23744 logs.go:123] Gathering logs for container status ...
I1227 01:49:30.887550   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:49:30.954021   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:49:30.954042   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:49:31.041885   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:49:31.041894   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:49:31.138768   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:49:31.138778   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:49:31.220100   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:49:31.220119   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:49:31.254167   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:49:31.254177   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:49:31.322587   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:49:31.322615   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:49:33.916368   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:49:38.916909   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:49:39.266699   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:49:39.302474   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:49:39.302513   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:49:39.327980   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:49:39.328061   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:49:39.353189   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:49:39.353221   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:49:39.378215   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:49:39.378258   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:49:39.444604   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:49:39.444694   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:49:39.473612   23744 logs.go:274] 0 containers: []
W1227 01:49:39.473621   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:49:39.473649   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:49:39.498658   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:49:39.498690   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:49:39.523079   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:49:39.523120   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:49:39.523136   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:49:39.531659   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:49:39.531670   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:49:39.635154   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:49:39.635181   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:49:39.665101   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:49:39.665114   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:49:39.734102   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:49:39.734121   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:49:39.793296   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:49:39.793305   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:49:39.878013   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:49:39.878022   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:49:39.961767   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:49:39.961783   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:49:40.033674   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:49:40.033698   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:49:40.119577   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:49:40.119591   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:49:40.190444   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:49:40.190463   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:49:40.279997   23744 logs.go:123] Gathering logs for container status ...
I1227 01:49:40.280006   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:49:40.310717   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:49:40.310729   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:49:42.883830   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:49:47.884057   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:49:48.266593   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:49:48.298017   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:49:48.298052   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:49:48.324612   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:49:48.324658   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:49:48.350787   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:49:48.350846   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:49:48.375752   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:49:48.375783   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:49:48.401404   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:49:48.401441   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:49:48.424959   23744 logs.go:274] 0 containers: []
W1227 01:49:48.424968   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:49:48.424994   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:49:48.488560   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:49:48.488628   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:49:48.515993   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:49:48.516012   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:49:48.516019   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:49:48.577466   23744 logs.go:123] Gathering logs for container status ...
I1227 01:49:48.577476   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:49:48.604790   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:49:48.604799   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:49:48.678721   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:49:48.678731   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:49:48.752156   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:49:48.752166   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:49:48.779916   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:49:48.779925   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:49:48.856350   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:49:48.856359   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:49:48.925121   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:49:48.925133   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:49:49.003955   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:49:49.003967   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:49:49.089653   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:49:49.089663   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:49:49.099159   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:49:49.099171   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:49:49.194089   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:49:49.194106   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:49:49.221154   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:49:49.221168   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:49:51.790427   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:49:56.791541   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:49:57.267331   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:49:57.300914   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:49:57.300947   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:49:57.326283   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:49:57.326319   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:49:57.353894   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:49:57.353926   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:49:57.378810   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:49:57.378864   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:49:57.403277   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:49:57.403329   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:49:57.427747   23744 logs.go:274] 0 containers: []
W1227 01:49:57.427756   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:49:57.427789   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:49:57.452590   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:49:57.452638   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:49:57.476661   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:49:57.476677   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:49:57.476688   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:49:57.502187   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:49:57.502196   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:49:57.570173   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:49:57.570195   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:49:57.674268   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:49:57.674278   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:49:57.753333   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:49:57.753364   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:49:57.785530   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:49:57.785540   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:49:57.858073   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:49:57.858084   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:49:57.926503   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:49:57.926524   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:49:58.010946   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:49:58.010957   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:49:58.071223   23744 logs.go:123] Gathering logs for container status ...
I1227 01:49:58.071244   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:49:58.096171   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:49:58.096181   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:49:58.182694   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:49:58.182703   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:49:58.191863   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:49:58.191876   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:50:00.772170   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:50:05.772569   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:50:06.266440   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:50:06.301340   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:50:06.301382   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:50:06.325837   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:50:06.325875   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:50:06.350511   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:50:06.350558   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:50:06.376426   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:50:06.376472   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:50:06.401779   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:50:06.401828   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:50:06.427055   23744 logs.go:274] 0 containers: []
W1227 01:50:06.427075   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:50:06.427115   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:50:06.451781   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:50:06.451819   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:50:06.476783   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:50:06.476803   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:50:06.476809   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:50:06.559387   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:50:06.559404   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:50:06.630647   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:50:06.630667   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:50:06.693212   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:50:06.693221   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:50:06.769377   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:50:06.769387   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:50:06.870841   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:50:06.870864   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:50:06.946920   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:50:06.946930   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:50:06.973703   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:50:06.973716   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:50:06.999669   23744 logs.go:123] Gathering logs for container status ...
I1227 01:50:06.999679   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:50:07.022421   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:50:07.022431   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:50:07.030921   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:50:07.030931   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:50:07.102537   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:50:07.102551   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:50:07.170224   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:50:07.170246   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:50:09.758854   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:50:14.759193   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:50:14.766349   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:50:14.832542   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:50:14.832601   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:50:14.904463   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:50:14.904544   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:50:14.976683   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:50:14.976757   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:50:15.004802   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:50:15.004834   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:50:15.028747   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:50:15.028797   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:50:15.052951   23744 logs.go:274] 0 containers: []
W1227 01:50:15.052960   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:50:15.053005   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:50:15.116560   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:50:15.116636   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:50:15.153733   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:50:15.153747   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:50:15.153754   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:50:15.162606   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:50:15.162617   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:50:15.239532   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:50:15.239541   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:50:15.266813   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:50:15.266827   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:50:15.349766   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:50:15.349776   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:50:15.434218   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:50:15.434227   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:50:15.508130   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:50:15.508139   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:50:15.535340   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:50:15.535353   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:50:15.618285   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:50:15.618298   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:50:15.690296   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:50:15.690315   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:50:15.761652   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:50:15.761670   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:50:15.814426   23744 logs.go:123] Gathering logs for container status ...
I1227 01:50:15.814436   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:50:15.877995   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:50:15.878013   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:50:18.475745   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:50:23.476422   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:50:23.766986   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:50:23.840725   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:50:23.840793   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:50:23.868290   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:50:23.868322   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:50:23.893107   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:50:23.893151   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:50:23.917707   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:50:23.917743   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:50:23.980637   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:50:23.980705   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:50:24.020705   23744 logs.go:274] 0 containers: []
W1227 01:50:24.020714   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:50:24.020741   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:50:24.049668   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:50:24.049702   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:50:24.074577   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:50:24.074593   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:50:24.074599   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:50:24.146453   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:50:24.146463   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:50:24.230225   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:50:24.230236   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:50:24.239748   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:50:24.239761   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:50:24.339063   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:50:24.339087   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:50:24.420855   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:50:24.420874   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:50:24.499701   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:50:24.499710   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:50:24.525728   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:50:24.525741   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:50:24.551325   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:50:24.551336   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:50:24.618398   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:50:24.618419   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:50:24.705332   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:50:24.705344   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:50:24.774783   23744 logs.go:123] Gathering logs for container status ...
I1227 01:50:24.774810   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:50:24.842187   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:50:24.842208   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:50:27.372759   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:50:32.373934   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:50:32.766584   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:50:32.836622   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:50:32.836693   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:50:32.864640   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:50:32.864734   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:50:32.889370   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:50:32.889412   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:50:32.917272   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:50:32.917310   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:50:32.941620   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:50:32.941661   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:50:32.967431   23744 logs.go:274] 0 containers: []
W1227 01:50:32.967441   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:50:32.967468   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:50:32.992492   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:50:32.992531   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:50:33.017636   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:50:33.017654   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:50:33.017660   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:50:33.102436   23744 logs.go:123] Gathering logs for container status ...
I1227 01:50:33.102445   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:50:33.127947   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:50:33.127959   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:50:33.214995   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:50:33.215004   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:50:33.286867   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:50:33.286875   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:50:33.359371   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:50:33.359398   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:50:33.388357   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:50:33.388370   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:50:33.414441   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:50:33.414451   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:50:33.488021   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:50:33.488030   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:50:33.547380   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:50:33.547399   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:50:33.601138   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:50:33.601157   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:50:33.709500   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:50:33.709510   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:50:33.787643   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:50:33.787672   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:50:36.361678   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:50:41.362849   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:50:41.767399   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:50:41.801124   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:50:41.801178   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:50:41.826360   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:50:41.826397   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:50:41.850687   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:50:41.850723   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:50:41.876532   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:50:41.876570   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:50:41.901333   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:50:41.901372   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:50:41.925353   23744 logs.go:274] 0 containers: []
W1227 01:50:41.925363   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:50:41.925387   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:50:41.951872   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:50:41.951912   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:50:42.020747   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:50:42.020788   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:50:42.020800   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:50:42.101030   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:50:42.101057   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:50:42.189622   23744 logs.go:123] Gathering logs for container status ...
I1227 01:50:42.189633   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:50:42.214388   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:50:42.214397   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:50:42.222786   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:50:42.222795   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:50:42.318645   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:50:42.318665   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:50:42.395131   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:50:42.395147   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:50:42.473091   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:50:42.473104   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:50:42.542305   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:50:42.542326   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:50:42.571313   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:50:42.571322   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:50:42.630188   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:50:42.630197   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:50:42.714272   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:50:42.714283   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:50:42.791468   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:50:42.791484   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:50:45.322524   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:50:50.323205   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:50:50.767005   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:50:50.801124   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:50:50.801164   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:50:50.864589   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:50:50.864652   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:50:50.900293   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:50:50.900358   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:50:50.926008   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:50:50.926041   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:50:50.950947   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:50:50.950986   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:50:50.975949   23744 logs.go:274] 0 containers: []
W1227 01:50:50.975957   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:50:50.975990   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:50:51.000499   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:50:51.000532   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:50:51.024697   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:50:51.024714   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:50:51.024753   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:50:51.103458   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:50:51.103474   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:50:51.173914   23744 logs.go:123] Gathering logs for container status ...
I1227 01:50:51.173931   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:50:51.197714   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:50:51.197724   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:50:51.249332   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:50:51.249352   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:50:51.359318   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:50:51.359343   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:50:51.439889   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:50:51.439905   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:50:51.471407   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:50:51.471417   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:50:51.541990   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:50:51.542008   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:50:51.632064   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:50:51.632075   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:50:51.691859   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:50:51.691884   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:50:51.780639   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:50:51.780649   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:50:51.861501   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:50:51.861522   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:50:54.392231   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:50:59.393247   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:50:59.393358   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1227 01:50:59.425348   23744 logs.go:274] 1 containers: [1103871b5423]
I1227 01:50:59.425392   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1227 01:50:59.455848   23744 logs.go:274] 1 containers: [609cdf37a1c1]
I1227 01:50:59.455913   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1227 01:50:59.480030   23744 logs.go:274] 2 containers: [6aad1ea55e6c cce109f078d6]
I1227 01:50:59.480087   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1227 01:50:59.505248   23744 logs.go:274] 1 containers: [3dc96b47ed4b]
I1227 01:50:59.505282   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1227 01:50:59.530093   23744 logs.go:274] 1 containers: [5ddd94a83b99]
I1227 01:50:59.530142   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1227 01:50:59.554384   23744 logs.go:274] 0 containers: []
W1227 01:50:59.554398   23744 logs.go:276] No container was found matching "kubernetes-dashboard"
I1227 01:50:59.554436   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1227 01:50:59.620738   23744 logs.go:274] 1 containers: [290b98eb74d2]
I1227 01:50:59.620806   23744 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1227 01:50:59.650656   23744 logs.go:274] 1 containers: [12f7a7a93cca]
I1227 01:50:59.650672   23744 logs.go:123] Gathering logs for kube-scheduler [3dc96b47ed4b] ...
I1227 01:50:59.650679   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3dc96b47ed4b"
I1227 01:50:59.731482   23744 logs.go:123] Gathering logs for kube-proxy [5ddd94a83b99] ...
I1227 01:50:59.731499   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5ddd94a83b99"
I1227 01:50:59.802452   23744 logs.go:123] Gathering logs for storage-provisioner [290b98eb74d2] ...
I1227 01:50:59.802471   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 290b98eb74d2"
I1227 01:50:59.830710   23744 logs.go:123] Gathering logs for dmesg ...
I1227 01:50:59.830720   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1227 01:50:59.839061   23744 logs.go:123] Gathering logs for describe nodes ...
I1227 01:50:59.839070   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1227 01:50:59.937248   23744 logs.go:123] Gathering logs for coredns [6aad1ea55e6c] ...
I1227 01:50:59.937257   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6aad1ea55e6c"
I1227 01:50:59.963913   23744 logs.go:123] Gathering logs for coredns [cce109f078d6] ...
I1227 01:50:59.963924   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cce109f078d6"
I1227 01:50:59.992192   23744 logs.go:123] Gathering logs for kube-controller-manager [12f7a7a93cca] ...
I1227 01:50:59.992206   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 12f7a7a93cca"
I1227 01:51:00.078270   23744 logs.go:123] Gathering logs for Docker ...
I1227 01:51:00.078282   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1227 01:51:00.135216   23744 logs.go:123] Gathering logs for container status ...
I1227 01:51:00.135233   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1227 01:51:00.160855   23744 logs.go:123] Gathering logs for kubelet ...
I1227 01:51:00.160865   23744 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1227 01:51:00.245715   23744 logs.go:123] Gathering logs for kube-apiserver [1103871b5423] ...
I1227 01:51:00.245724   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1103871b5423"
I1227 01:51:00.318703   23744 logs.go:123] Gathering logs for etcd [609cdf37a1c1] ...
I1227 01:51:00.318713   23744 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 609cdf37a1c1"
I1227 01:51:02.892189   23744 api_server.go:240] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1227 01:51:07.894009   23744 api_server.go:256] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1227 01:51:07.901401   23744 out.go:177] 
W1227 01:51:07.902782   23744 out.go:239] ‚ùå  Fermeture en raison de GUEST_START : wait 6m0s for node: wait for healthy API server: apiserver healthz never reported healthy: timed out waiting for the condition
W1227 01:51:07.902808   23744 out.go:239] 
W1227 01:51:07.905379   23744 out.go:239] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                                  [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  Si les conseils ci-dessus ne vous aident pas, veuillez nous en informer :                 [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                                  [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                  [31m‚îÇ[0m
[31m‚îÇ[0m    Veuillez ex√©cuter `minikube logs --file=logs.txt` et attachez logs.txt au probl√®me GitHub.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                  [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I1227 01:51:07.906462   23744 out.go:177] 

* 
* ==> Docker <==
* -- Logs begin at Tue 2022-12-27 00:43:30 UTC, end at Tue 2022-12-27 00:54:58 UTC. --
Dec 27 00:43:30 minikube systemd[1]: Starting Docker Application Container Engine...
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.608533649Z" level=info msg="Starting up"
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.609920009Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.609989106Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.610033925Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.610066834Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.612058818Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.612074951Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.612085107Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.612091064Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.619435273Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.634322594Z" level=info msg="Loading containers: start."
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.681389320Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.710669707Z" level=info msg="Loading containers: done."
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.719602567Z" level=info msg="Docker daemon" commit=4433bf6 graphdriver(s)=overlay2 version=20.10.15
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.719701554Z" level=info msg="Daemon has completed initialization"
Dec 27 00:43:30 minikube systemd[1]: Started Docker Application Container Engine.
Dec 27 00:43:30 minikube dockerd[123]: time="2022-12-27T00:43:30.760797989Z" level=info msg="API listen on /run/docker.sock"
Dec 27 00:43:34 minikube systemd[1]: docker.service: Current command vanished from the unit file, execution of the command list won't be resumed.
Dec 27 00:43:34 minikube systemd[1]: Stopping Docker Application Container Engine...
Dec 27 00:43:34 minikube dockerd[123]: time="2022-12-27T00:43:34.485869366Z" level=info msg="Processing signal 'terminated'"
Dec 27 00:43:34 minikube dockerd[123]: time="2022-12-27T00:43:34.486434633Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Dec 27 00:43:34 minikube dockerd[123]: time="2022-12-27T00:43:34.486889336Z" level=info msg="Daemon shutdown complete"
Dec 27 00:43:34 minikube systemd[1]: docker.service: Succeeded.
Dec 27 00:43:34 minikube systemd[1]: Stopped Docker Application Container Engine.
Dec 27 00:43:34 minikube systemd[1]: Starting Docker Application Container Engine...
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.527391446Z" level=info msg="Starting up"
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.528761963Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.528790189Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.528807298Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.528817223Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.529629700Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.529642302Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.529651133Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.529656331Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.552930736Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.562182290Z" level=info msg="Loading containers: start."
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.641462627Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.664593096Z" level=info msg="Loading containers: done."
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.672930933Z" level=info msg="Docker daemon" commit=4433bf6 graphdriver(s)=overlay2 version=20.10.15
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.672978848Z" level=info msg="Daemon has completed initialization"
Dec 27 00:43:34 minikube systemd[1]: Started Docker Application Container Engine.
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.693358568Z" level=info msg="API listen on [::]:2376"
Dec 27 00:43:34 minikube dockerd[374]: time="2022-12-27T00:43:34.697838608Z" level=info msg="API listen on /var/run/docker.sock"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
290b98eb74d2a       6e38f40d628db       8 minutes ago       Running             storage-provisioner       0                   ac131a8c61d7c
6aad1ea55e6c6       a4ca41631cc7a       10 minutes ago      Running             coredns                   0                   142c8eb888f93
cce109f078d6a       a4ca41631cc7a       10 minutes ago      Running             coredns                   0                   895652c94c978
5ddd94a83b998       4c03754524064       10 minutes ago      Running             kube-proxy                0                   39c972ba4942b
3dc96b47ed4be       595f327f224a4       11 minutes ago      Running             kube-scheduler            0                   36207852126c5
1103871b5423c       8fa62c12256df       11 minutes ago      Running             kube-apiserver            0                   f5ec28c1aef44
609cdf37a1c1e       25f8c7f3da61c       11 minutes ago      Running             etcd                      0                   01e1d80ac02db
12f7a7a93cca9       df7b72818ad2e       11 minutes ago      Running             kube-controller-manager   0                   251a404adb5e7

* 
* ==> coredns [6aad1ea55e6c] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = db32ca3650231d74073ff4cf814959a7
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] Reloading
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] plugin/reload: Running configuration MD5 = 7cb80d9b13c0af3fa1ba04fc3eef5f89
[INFO] Reloading complete

* 
* ==> coredns [cce109f078d6] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = db32ca3650231d74073ff4cf814959a7
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] Reloading
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] plugin/reload: Running configuration MD5 = 7cb80d9b13c0af3fa1ba04fc3eef5f89
[INFO] Reloading complete

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f0a6d95bdb2f2b83c4f952383fe29de03c269eab
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_12_27T01_43_48_0700
                    minikube.k8s.io/version=v1.26.0-beta.1
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 27 Dec 2022 00:43:45 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 27 Dec 2022 00:54:52 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 27 Dec 2022 00:54:11 +0000   Tue, 27 Dec 2022 00:43:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 27 Dec 2022 00:54:11 +0000   Tue, 27 Dec 2022 00:43:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 27 Dec 2022 00:54:11 +0000   Tue, 27 Dec 2022 00:43:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 27 Dec 2022 00:54:11 +0000   Tue, 27 Dec 2022 00:43:59 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  65739308Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3853628Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  65739308Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3853628Ki
  pods:               110
System Info:
  Machine ID:                 1729fd8b7c184ebda96a08181510f608
  System UUID:                1729fd8b7c184ebda96a08181510f608
  Boot ID:                    c894e2bd-56e6-477d-8180-e706619cdd43
  Kernel Version:             5.15.49-linuxkit
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.15
  Kubelet Version:            v1.23.6
  Kube-Proxy Version:         v1.23.6
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-64897985d-g4c2w             100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     10m
  kube-system                 coredns-64897985d-hvlbm             100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     10m
  kube-system                 etcd-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         11m
  kube-system                 kube-apiserver-minikube             250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 kube-controller-manager-minikube    200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 kube-proxy-vkzld                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 kube-scheduler-minikube             100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8m7s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             240Mi (6%!)(MISSING)  340Mi (9%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age   From        Message
  ----    ------                   ----  ----        -------
  Normal  Starting                 10m   kube-proxy  
  Normal  NodeAllocatableEnforced  11m   kubelet     Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  11m   kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m   kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     11m   kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 11m   kubelet     Starting kubelet.
  Normal  NodeReady                10m   kubelet     Node minikube status is now: NodeReady

* 
* ==> dmesg <==
* [Dec26 23:20]  #2
[  +0.002057]  #3
[  +0.220518] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.021585] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.031695] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[Dec26 23:22] grpcfuse: loading out-of-tree module taints kernel.

* 
* ==> etcd [609cdf37a1c1] <==
* {"level":"info","ts":"2022-12-27T00:43:44.322Z","caller":"etcdmain/etcd.go:72","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.58.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--initial-advertise-peer-urls=https://192.168.58.2:2380","--initial-cluster=minikube=https://192.168.58.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.58.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.58.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-12-27T00:43:44.322Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2022-12-27T00:43:44.322Z","caller":"embed/etcd.go:478","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-12-27T00:43:44.323Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"]}
{"level":"info","ts":"2022-12-27T00:43:44.323Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.1","git-sha":"e8732fb5f","go-version":"go1.16.3","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.58.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":false,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-12-27T00:43:44.329Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"6.601085ms"}
{"level":"info","ts":"2022-12-27T00:43:44.335Z","caller":"etcdserver/raft.go:448","msg":"starting local member","local-member-id":"b2c6679ac05f2cf1","cluster-id":"3a56e4ca95e2355c"}
{"level":"info","ts":"2022-12-27T00:43:44.335Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=()"}
{"level":"info","ts":"2022-12-27T00:43:44.335Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 0"}
{"level":"info","ts":"2022-12-27T00:43:44.335Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b2c6679ac05f2cf1 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2022-12-27T00:43:44.335Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 1"}
{"level":"info","ts":"2022-12-27T00:43:44.335Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"warn","ts":"2022-12-27T00:43:44.338Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-12-27T00:43:44.340Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2022-12-27T00:43:44.341Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-12-27T00:43:44.344Z","caller":"etcdserver/server.go:843","msg":"starting etcd server","local-member-id":"b2c6679ac05f2cf1","local-server-version":"3.5.1","cluster-version":"to_be_decided"}
{"level":"info","ts":"2022-12-27T00:43:44.345Z","caller":"etcdserver/server.go:728","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b2c6679ac05f2cf1","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-12-27T00:43:44.345Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"info","ts":"2022-12-27T00:43:44.345Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","added-peer-id":"b2c6679ac05f2cf1","added-peer-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2022-12-27T00:43:44.346Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-12-27T00:43:44.346Z","caller":"embed/etcd.go:580","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2022-12-27T00:43:44.346Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2022-12-27T00:43:44.346Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"b2c6679ac05f2cf1","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-12-27T00:43:44.346Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-12-27T00:43:44.535Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 is starting a new election at term 1"}
{"level":"info","ts":"2022-12-27T00:43:44.535Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became pre-candidate at term 1"}
{"level":"info","ts":"2022-12-27T00:43:44.535Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgPreVoteResp from b2c6679ac05f2cf1 at term 1"}
{"level":"info","ts":"2022-12-27T00:43:44.535Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became candidate at term 2"}
{"level":"info","ts":"2022-12-27T00:43:44.535Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgVoteResp from b2c6679ac05f2cf1 at term 2"}
{"level":"info","ts":"2022-12-27T00:43:44.535Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became leader at term 2"}
{"level":"info","ts":"2022-12-27T00:43:44.535Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b2c6679ac05f2cf1 elected leader b2c6679ac05f2cf1 at term 2"}
{"level":"info","ts":"2022-12-27T00:43:44.536Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"b2c6679ac05f2cf1","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/b2c6679ac05f2cf1/attributes","cluster-id":"3a56e4ca95e2355c","publish-timeout":"7s"}
{"level":"info","ts":"2022-12-27T00:43:44.536Z","caller":"etcdserver/server.go:2476","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-27T00:43:44.536Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-27T00:43:44.537Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.58.2:2379"}
{"level":"info","ts":"2022-12-27T00:43:44.537Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-27T00:43:44.538Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-12-27T00:43:44.540Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-27T00:43:44.540Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-27T00:43:44.540Z","caller":"etcdserver/server.go:2500","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-27T00:43:44.541Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2022-12-27T00:43:44.544Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"warn","ts":"2022-12-27T00:44:01.771Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"117.080968ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/root-ca-cert-publisher\" ","response":"range_response_count:1 size:263"}
{"level":"info","ts":"2022-12-27T00:44:01.771Z","caller":"traceutil/trace.go:171","msg":"trace[613500841] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/root-ca-cert-publisher; range_end:; response_count:1; response_revision:405; }","duration":"117.253334ms","start":"2022-12-27T00:44:01.654Z","end":"2022-12-27T00:44:01.771Z","steps":["trace[613500841] 'agreement among raft nodes before linearized reading'  (duration: 96.788217ms)","trace[613500841] 'range keys from in-memory index tree'  (duration: 20.230278ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-27T00:44:01.771Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"112.893403ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/kube-proxy\" ","response":"range_response_count:1 size:226"}
{"level":"info","ts":"2022-12-27T00:44:01.772Z","caller":"traceutil/trace.go:171","msg":"trace[496941156] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/kube-proxy; range_end:; response_count:1; response_revision:405; }","duration":"113.597073ms","start":"2022-12-27T00:44:01.658Z","end":"2022-12-27T00:44:01.772Z","steps":["trace[496941156] 'agreement among raft nodes before linearized reading'  (duration: 92.573709ms)","trace[496941156] 'range keys from in-memory index tree'  (duration: 20.261261ms)"],"step_count":2}
{"level":"info","ts":"2022-12-27T00:53:44.666Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":579}
{"level":"info","ts":"2022-12-27T00:53:44.668Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":579,"took":"715.616¬µs"}

* 
* ==> kernel <==
*  00:54:58 up  1:34,  0 users,  load average: 0.20, 0.24, 0.15
Linux minikube 5.15.49-linuxkit #1 SMP Tue Sep 13 07:51:46 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.4 LTS"

* 
* ==> kube-apiserver [1103871b5423] <==
* W1227 00:43:45.133091       1 genericapiserver.go:538] Skipping API apps/v1beta2 because it has no resources.
W1227 00:43:45.133113       1 genericapiserver.go:538] Skipping API apps/v1beta1 because it has no resources.
W1227 00:43:45.134630       1 genericapiserver.go:538] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I1227 00:43:45.137873       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1227 00:43:45.137905       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1227 00:43:45.163888       1 genericapiserver.go:538] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1227 00:43:45.713222       1 secure_serving.go:266] Serving securely on [::]:8443
I1227 00:43:45.713403       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1227 00:43:45.717351       1 available_controller.go:491] Starting AvailableConditionController
I1227 00:43:45.717366       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1227 00:43:45.717428       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1227 00:43:45.717424       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I1227 00:43:45.717456       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1227 00:43:45.734777       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1227 00:43:45.717548       1 controller.go:83] Starting OpenAPI AggregationController
I1227 00:43:45.717571       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1227 00:43:45.717907       1 controller.go:85] Starting OpenAPI controller
I1227 00:43:45.717919       1 dynamic_serving_content.go:131] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1227 00:43:45.717921       1 naming_controller.go:291] Starting NamingConditionController
I1227 00:43:45.717928       1 establishing_controller.go:76] Starting EstablishingController
I1227 00:43:45.717952       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1227 00:43:45.717958       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1227 00:43:45.717965       1 crd_finalizer.go:266] Starting CRDFinalizer
I1227 00:43:45.718075       1 autoregister_controller.go:141] Starting autoregister controller
I1227 00:43:45.742249       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1227 00:43:45.718970       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1227 00:43:45.742282       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I1227 00:43:45.718983       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1227 00:43:45.719050       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1227 00:43:45.719055       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1227 00:43:45.717568       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1227 00:43:45.734102       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1227 00:43:45.745979       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I1227 00:43:45.775279       1 controller.go:611] quota admission added evaluator for: namespaces
I1227 00:43:45.817418       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1227 00:43:45.821775       1 apf_controller.go:322] Running API Priority and Fairness config worker
I1227 00:43:45.835390       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1227 00:43:45.842597       1 cache.go:39] Caches are synced for autoregister controller
I1227 00:43:45.842597       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I1227 00:43:45.846146       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I1227 00:43:45.849229       1 shared_informer.go:247] Caches are synced for node_authorizer 
I1227 00:43:46.713279       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I1227 00:43:46.713321       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1227 00:43:46.724216       1 storage_scheduling.go:93] created PriorityClass system-node-critical with value 2000001000
I1227 00:43:46.740331       1 storage_scheduling.go:93] created PriorityClass system-cluster-critical with value 2000000000
I1227 00:43:46.740383       1 storage_scheduling.go:109] all system priority classes are created successfully or already exist.
I1227 00:43:46.983526       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1227 00:43:47.010360       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1227 00:43:47.094514       1 alloc.go:329] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1227 00:43:47.102483       1 lease.go:233] Resetting endpoints for master service "kubernetes" to [192.168.58.2]
I1227 00:43:47.103909       1 controller.go:611] quota admission added evaluator for: endpoints
I1227 00:43:47.109411       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1227 00:43:47.859010       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I1227 00:43:48.641697       1 controller.go:611] quota admission added evaluator for: deployments.apps
I1227 00:43:48.650884       1 alloc.go:329] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1227 00:43:48.689501       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I1227 00:43:48.779646       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I1227 00:44:01.262736       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I1227 00:44:01.436817       1 controller.go:611] quota admission added evaluator for: controllerrevisions.apps
I1227 00:44:04.694655       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io

* 
* ==> kube-controller-manager [12f7a7a93cca] <==
* I1227 00:44:01.200738       1 controllermanager.go:605] Started "persistentvolume-binder"
I1227 00:44:01.201136       1 pv_controller_base.go:310] Starting persistent volume controller
I1227 00:44:01.201182       1 shared_informer.go:240] Waiting for caches to sync for persistent volume
I1227 00:44:01.204514       1 shared_informer.go:240] Waiting for caches to sync for resource quota
W1227 00:44:01.210520       1 actual_state_of_world.go:539] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I1227 00:44:01.225588       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I1227 00:44:01.251424       1 shared_informer.go:247] Caches are synced for HPA 
I1227 00:44:01.252578       1 shared_informer.go:247] Caches are synced for PVC protection 
I1227 00:44:01.253673       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I1227 00:44:01.253690       1 shared_informer.go:247] Caches are synced for GC 
I1227 00:44:01.253713       1 shared_informer.go:247] Caches are synced for deployment 
I1227 00:44:01.253721       1 shared_informer.go:247] Caches are synced for cronjob 
I1227 00:44:01.257008       1 shared_informer.go:247] Caches are synced for ReplicationController 
I1227 00:44:01.259582       1 shared_informer.go:247] Caches are synced for stateful set 
I1227 00:44:01.261545       1 shared_informer.go:247] Caches are synced for service account 
I1227 00:44:01.265100       1 event.go:294] "Event occurred" object="kube-system/coredns" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-64897985d to 2"
I1227 00:44:01.278679       1 shared_informer.go:247] Caches are synced for job 
I1227 00:44:01.279793       1 shared_informer.go:247] Caches are synced for node 
I1227 00:44:01.279820       1 range_allocator.go:173] Starting range CIDR allocator
I1227 00:44:01.279826       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I1227 00:44:01.279849       1 shared_informer.go:247] Caches are synced for cidrallocator 
I1227 00:44:01.283342       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I1227 00:44:01.283376       1 shared_informer.go:247] Caches are synced for taint 
I1227 00:44:01.284338       1 node_lifecycle_controller.go:1397] Initializing eviction metric for zone: 
W1227 00:44:01.284488       1 node_lifecycle_controller.go:1012] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1227 00:44:01.284603       1 node_lifecycle_controller.go:1213] Controller detected that zone  is now in state Normal.
I1227 00:44:01.284663       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I1227 00:44:01.284830       1 event.go:294] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1227 00:44:01.289971       1 shared_informer.go:247] Caches are synced for PV protection 
I1227 00:44:01.293186       1 shared_informer.go:247] Caches are synced for TTL after finished 
I1227 00:44:01.297345       1 shared_informer.go:247] Caches are synced for TTL 
I1227 00:44:01.299629       1 shared_informer.go:247] Caches are synced for namespace 
I1227 00:44:01.300754       1 shared_informer.go:247] Caches are synced for expand 
I1227 00:44:01.301942       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I1227 00:44:01.301952       1 shared_informer.go:247] Caches are synced for daemon sets 
I1227 00:44:01.302251       1 shared_informer.go:247] Caches are synced for endpoint 
I1227 00:44:01.302339       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I1227 00:44:01.302408       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I1227 00:44:01.302749       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I1227 00:44:01.303120       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I1227 00:44:01.303271       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I1227 00:44:01.304251       1 shared_informer.go:247] Caches are synced for ephemeral 
I1227 00:44:01.304257       1 shared_informer.go:247] Caches are synced for crt configmap 
I1227 00:44:01.307779       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I1227 00:44:01.314054       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I1227 00:44:01.343353       1 range_allocator.go:374] Set node minikube PodCIDR to [10.244.0.0/24]
I1227 00:44:01.401424       1 shared_informer.go:247] Caches are synced for persistent volume 
I1227 00:44:01.423932       1 shared_informer.go:247] Caches are synced for disruption 
I1227 00:44:01.423956       1 disruption.go:371] Sending events to api server.
I1227 00:44:01.471924       1 shared_informer.go:247] Caches are synced for resource quota 
I1227 00:44:01.486740       1 shared_informer.go:247] Caches are synced for attach detach 
I1227 00:44:01.497803       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-vkzld"
I1227 00:44:01.504716       1 shared_informer.go:247] Caches are synced for resource quota 
I1227 00:44:01.582044       1 event.go:294] "Event occurred" object="kube-system/coredns-64897985d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-64897985d-g4c2w"
I1227 00:44:01.633891       1 event.go:294] "Event occurred" object="kube-system/coredns-64897985d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-64897985d-hvlbm"
W1227 00:44:01.647325       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kube-system/kube-dns", retrying. Error: EndpointSlice informer cache is out of date
I1227 00:44:01.814339       1 event.go:294] "Event occurred" object="kube-dns" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToCreateEndpoint" message="Failed to create endpoint for service kube-system/kube-dns: endpoints \"kube-dns\" already exists"
I1227 00:44:01.926208       1 shared_informer.go:247] Caches are synced for garbage collector 
I1227 00:44:02.002594       1 shared_informer.go:247] Caches are synced for garbage collector 
I1227 00:44:02.002640       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage

* 
* ==> kube-proxy [5ddd94a83b99] <==
* I1227 00:44:04.441349       1 node.go:163] Successfully retrieved node IP: 192.168.58.2
I1227 00:44:04.441462       1 server_others.go:138] "Detected node IP" address="192.168.58.2"
I1227 00:44:04.441661       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1227 00:44:04.684032       1 server_others.go:206] "Using iptables Proxier"
I1227 00:44:04.684078       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1227 00:44:04.684092       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1227 00:44:04.684135       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1227 00:44:04.685446       1 server.go:656] "Version info" version="v1.23.6"
I1227 00:44:04.687829       1 config.go:226] "Starting endpoint slice config controller"
I1227 00:44:04.688183       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1227 00:44:04.688537       1 config.go:317] "Starting service config controller"
I1227 00:44:04.688671       1 shared_informer.go:240] Waiting for caches to sync for service config
I1227 00:44:04.789205       1 shared_informer.go:247] Caches are synced for service config 
I1227 00:44:04.789309       1 shared_informer.go:247] Caches are synced for endpoint slice config 

* 
* ==> kube-scheduler [3dc96b47ed4b] <==
* I1227 00:43:44.596990       1 serving.go:348] Generated self-signed cert in-memory
W1227 00:43:45.735857       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1227 00:43:45.735884       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1227 00:43:45.735890       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W1227 00:43:45.735895       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1227 00:43:45.762318       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.6"
I1227 00:43:45.763326       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I1227 00:43:45.763444       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1227 00:43:45.763458       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1227 00:43:45.764111       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1227 00:43:45.764297       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1227 00:43:45.764411       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1227 00:43:45.764995       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1227 00:43:45.765031       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1227 00:43:45.765288       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1227 00:43:45.765307       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1227 00:43:45.765295       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1227 00:43:45.765364       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1227 00:43:45.765370       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1227 00:43:45.765404       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1227 00:43:45.765413       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1227 00:43:45.765443       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1227 00:43:45.765481       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1227 00:43:45.765497       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1227 00:43:45.765535       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1227 00:43:45.765544       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1227 00:43:45.765551       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1227 00:43:45.765569       1 reflector.go:324] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1227 00:43:45.765603       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1227 00:43:45.765451       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1227 00:43:45.765444       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1227 00:43:45.765640       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1227 00:43:45.765655       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1227 00:43:45.765678       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1227 00:43:45.765710       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1227 00:43:45.765716       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1227 00:43:45.765720       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1227 00:43:45.765723       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1227 00:43:45.765735       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1227 00:43:45.765745       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1227 00:43:46.625170       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1227 00:43:46.625226       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1227 00:43:46.704593       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1227 00:43:46.704646       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1227 00:43:46.827126       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1227 00:43:46.827188       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1227 00:43:46.854866       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1227 00:43:46.854997       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1227 00:43:46.881003       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1227 00:43:46.881073       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1227 00:43:46.892720       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1227 00:43:46.892741       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1227 00:43:47.032949       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E1227 00:43:47.236679       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
I1227 00:43:47.364244       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
E1227 00:43:47.637223       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"

* 
* ==> kubelet <==
* -- Logs begin at Tue 2022-12-27 00:43:30 UTC, end at Tue 2022-12-27 00:54:59 UTC. --
Dec 27 00:43:48 minikube kubelet[1748]: E1227 00:43:48.905195    1748 kubelet.go:2040] "Skipping pod synchronization" err="PLEG is not healthy: pleg has yet to be successful"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.006279    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.006407    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.006474    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.006509    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:43:49 minikube kubelet[1748]: E1227 00:43:49.019699    1748 kubelet.go:1742] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: E1227 00:43:49.019979    1748 kubelet.go:1742] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075358    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9f91abc59d9c62287c841f47ae9b9cc2-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9f91abc59d9c62287c841f47ae9b9cc2\") " pod="kube-system/kube-apiserver-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075402    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/0bb2ecd05a9af7279aca727e6d0f233c-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"0bb2ecd05a9af7279aca727e6d0f233c\") " pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075423    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/0bb2ecd05a9af7279aca727e6d0f233c-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0bb2ecd05a9af7279aca727e6d0f233c\") " pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075444    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0bb2ecd05a9af7279aca727e6d0f233c-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0bb2ecd05a9af7279aca727e6d0f233c\") " pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075463    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/9f91abc59d9c62287c841f47ae9b9cc2-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"9f91abc59d9c62287c841f47ae9b9cc2\") " pod="kube-system/kube-apiserver-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075499    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/0bb2ecd05a9af7279aca727e6d0f233c-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"0bb2ecd05a9af7279aca727e6d0f233c\") " pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075533    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0bb2ecd05a9af7279aca727e6d0f233c-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0bb2ecd05a9af7279aca727e6d0f233c\") " pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075555    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/482305b50d4b0d3dbab5b9f1be9b0b18-etcd-data\") pod \"etcd-minikube\" (UID: \"482305b50d4b0d3dbab5b9f1be9b0b18\") " pod="kube-system/etcd-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075571    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9f91abc59d9c62287c841f47ae9b9cc2-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9f91abc59d9c62287c841f47ae9b9cc2\") " pod="kube-system/kube-apiserver-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075583    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0bb2ecd05a9af7279aca727e6d0f233c-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0bb2ecd05a9af7279aca727e6d0f233c\") " pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075595    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/482305b50d4b0d3dbab5b9f1be9b0b18-etcd-certs\") pod \"etcd-minikube\" (UID: \"482305b50d4b0d3dbab5b9f1be9b0b18\") " pod="kube-system/etcd-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075608    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/9f91abc59d9c62287c841f47ae9b9cc2-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"9f91abc59d9c62287c841f47ae9b9cc2\") " pod="kube-system/kube-apiserver-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075643    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9f91abc59d9c62287c841f47ae9b9cc2-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9f91abc59d9c62287c841f47ae9b9cc2\") " pod="kube-system/kube-apiserver-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075705    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/0bb2ecd05a9af7279aca727e6d0f233c-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0bb2ecd05a9af7279aca727e6d0f233c\") " pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.075744    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/638de042f266ff18c5036c24a6b9b230-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"638de042f266ff18c5036c24a6b9b230\") " pod="kube-system/kube-scheduler-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: E1227 00:43:49.165014    1748 kubelet.go:1742] "Failed creating a mirror pod for" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.759866    1748 apiserver.go:52] "Watching apiserver"
Dec 27 00:43:49 minikube kubelet[1748]: I1227 00:43:49.981157    1748 reconciler.go:157] "Reconciler: start to sync state"
Dec 27 00:43:50 minikube kubelet[1748]: E1227 00:43:50.383050    1748 kubelet.go:1742] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Dec 27 00:43:50 minikube kubelet[1748]: E1227 00:43:50.577958    1748 kubelet.go:1742] "Failed creating a mirror pod for" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Dec 27 00:43:50 minikube kubelet[1748]: E1227 00:43:50.766084    1748 kubelet.go:1742] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Dec 27 00:43:50 minikube kubelet[1748]: I1227 00:43:50.960362    1748 request.go:665] Waited for 1.028532338s due to client-side throttling, not priority and fairness, request: POST:https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods
Dec 27 00:43:50 minikube kubelet[1748]: E1227 00:43:50.972581    1748 kubelet.go:1742] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.355946    1748 kuberuntime_manager.go:1105] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.357183    1748 docker_service.go:364] "Docker cri received runtime config" runtimeConfig="&RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.357514    1748 kubelet_network.go:76] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.521040    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.557459    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/18c9c099-8d0c-45aa-b133-9e5fe6d18b8c-kube-proxy\") pod \"kube-proxy-vkzld\" (UID: \"18c9c099-8d0c-45aa-b133-9e5fe6d18b8c\") " pod="kube-system/kube-proxy-vkzld"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.557531    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dxcvh\" (UniqueName: \"kubernetes.io/projected/18c9c099-8d0c-45aa-b133-9e5fe6d18b8c-kube-api-access-dxcvh\") pod \"kube-proxy-vkzld\" (UID: \"18c9c099-8d0c-45aa-b133-9e5fe6d18b8c\") " pod="kube-system/kube-proxy-vkzld"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.557578    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/18c9c099-8d0c-45aa-b133-9e5fe6d18b8c-lib-modules\") pod \"kube-proxy-vkzld\" (UID: \"18c9c099-8d0c-45aa-b133-9e5fe6d18b8c\") " pod="kube-system/kube-proxy-vkzld"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.557602    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/18c9c099-8d0c-45aa-b133-9e5fe6d18b8c-xtables-lock\") pod \"kube-proxy-vkzld\" (UID: \"18c9c099-8d0c-45aa-b133-9e5fe6d18b8c\") " pod="kube-system/kube-proxy-vkzld"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.631741    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.647377    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.758692    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/6f3db89e-93d6-4e38-b6bf-a064de28a991-config-volume\") pod \"coredns-64897985d-g4c2w\" (UID: \"6f3db89e-93d6-4e38-b6bf-a064de28a991\") " pod="kube-system/coredns-64897985d-g4c2w"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.758808    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5d4tw\" (UniqueName: \"kubernetes.io/projected/6f3db89e-93d6-4e38-b6bf-a064de28a991-kube-api-access-5d4tw\") pod \"coredns-64897985d-g4c2w\" (UID: \"6f3db89e-93d6-4e38-b6bf-a064de28a991\") " pod="kube-system/coredns-64897985d-g4c2w"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.758868    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xw7fz\" (UniqueName: \"kubernetes.io/projected/668105c9-cad8-4129-9f7a-d898f09282a8-kube-api-access-xw7fz\") pod \"coredns-64897985d-hvlbm\" (UID: \"668105c9-cad8-4129-9f7a-d898f09282a8\") " pod="kube-system/coredns-64897985d-hvlbm"
Dec 27 00:44:01 minikube kubelet[1748]: I1227 00:44:01.758945    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/668105c9-cad8-4129-9f7a-d898f09282a8-config-volume\") pod \"coredns-64897985d-hvlbm\" (UID: \"668105c9-cad8-4129-9f7a-d898f09282a8\") " pod="kube-system/coredns-64897985d-hvlbm"
Dec 27 00:44:01 minikube kubelet[1748]: E1227 00:44:01.794447    1748 projected.go:293] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Dec 27 00:44:01 minikube kubelet[1748]: E1227 00:44:01.794504    1748 projected.go:199] Error preparing data for projected volume kube-api-access-dxcvh for pod kube-system/kube-proxy-vkzld: configmap "kube-root-ca.crt" not found
Dec 27 00:44:01 minikube kubelet[1748]: E1227 00:44:01.794611    1748 nestedpendingoperations.go:335] Operation for "{volumeName:kubernetes.io/projected/18c9c099-8d0c-45aa-b133-9e5fe6d18b8c-kube-api-access-dxcvh podName:18c9c099-8d0c-45aa-b133-9e5fe6d18b8c nodeName:}" failed. No retries permitted until 2022-12-27 00:44:02.294577642 +0000 UTC m=+13.669851851 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-dxcvh" (UniqueName: "kubernetes.io/projected/18c9c099-8d0c-45aa-b133-9e5fe6d18b8c-kube-api-access-dxcvh") pod "kube-proxy-vkzld" (UID: "18c9c099-8d0c-45aa-b133-9e5fe6d18b8c") : configmap "kube-root-ca.crt" not found
Dec 27 00:44:03 minikube kubelet[1748]: I1227 00:44:03.587641    1748 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-64897985d-hvlbm through plugin: invalid network status for"
Dec 27 00:44:04 minikube kubelet[1748]: I1227 00:44:04.289065    1748 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="142c8eb888f934e686c933e2478339ae462a23b1be773307c26335965835355f"
Dec 27 00:44:04 minikube kubelet[1748]: I1227 00:44:04.290961    1748 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-64897985d-g4c2w through plugin: invalid network status for"
Dec 27 00:44:04 minikube kubelet[1748]: I1227 00:44:04.461543    1748 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="39c972ba4942baa14798e7a55f8f8cacad85ca671ede4b1bdc9308b258dfc8a0"
Dec 27 00:44:04 minikube kubelet[1748]: I1227 00:44:04.465392    1748 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-64897985d-hvlbm through plugin: invalid network status for"
Dec 27 00:44:04 minikube kubelet[1748]: I1227 00:44:04.635040    1748 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="895652c94c978e2701e8f76593a11bdea3149f8c96071c36d2e431b4d212a6f6"
Dec 27 00:44:05 minikube kubelet[1748]: I1227 00:44:05.648811    1748 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-64897985d-hvlbm through plugin: invalid network status for"
Dec 27 00:44:05 minikube kubelet[1748]: I1227 00:44:05.654474    1748 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-64897985d-g4c2w through plugin: invalid network status for"
Dec 27 00:46:51 minikube kubelet[1748]: I1227 00:46:51.814429    1748 topology_manager.go:200] "Topology Admit Handler"
Dec 27 00:46:51 minikube kubelet[1748]: I1227 00:46:51.991508    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/1bfd42f3-7a8d-44c9-8d43-5cf8ff34060c-tmp\") pod \"storage-provisioner\" (UID: \"1bfd42f3-7a8d-44c9-8d43-5cf8ff34060c\") " pod="kube-system/storage-provisioner"
Dec 27 00:46:51 minikube kubelet[1748]: I1227 00:46:51.991620    1748 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b8kvl\" (UniqueName: \"kubernetes.io/projected/1bfd42f3-7a8d-44c9-8d43-5cf8ff34060c-kube-api-access-b8kvl\") pod \"storage-provisioner\" (UID: \"1bfd42f3-7a8d-44c9-8d43-5cf8ff34060c\") " pod="kube-system/storage-provisioner"
Dec 27 00:48:48 minikube kubelet[1748]: W1227 00:48:48.818817    1748 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 27 00:53:48 minikube kubelet[1748]: W1227 00:53:48.820507    1748 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [290b98eb74d2] <==
* I1227 00:46:52.347797       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1227 00:46:52.356521       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1227 00:46:52.356710       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1227 00:46:52.366612       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1227 00:46:52.366747       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e99baff7-74eb-48cb-9a10-5ab3b1d1e802!
I1227 00:46:52.367131       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"0ec428ff-1492-4e21-a3d5-fbdcfbb6320a", APIVersion:"v1", ResourceVersion:"499", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e99baff7-74eb-48cb-9a10-5ab3b1d1e802 became leader
I1227 00:46:52.467139       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e99baff7-74eb-48cb-9a10-5ab3b1d1e802!

